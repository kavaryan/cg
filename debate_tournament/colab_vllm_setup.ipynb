{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab vLLM Setup with ngrok and GPU Monitoring\n",
    "\n",
    "This notebook sets up a vLLM server for a small Qwen model (0.5B preferred, fallback to 1.5B), tunnels it with ngrok, and monitors GPU usage with nvidia-smi (nvtop alternative for Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install vllm pyngrok torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ngrok (replace YOUR_NGROK_AUTH_TOKEN with your actual token)\n",
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")  # Get from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "\n",
    "# Start ngrok tunnel for port 8000\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"Public URL: {public_url}\")\n",
    "print(f\"Set VLLM_BASE_URL={public_url}/v1 in your local environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and run vLLM server with Qwen 0.5B model\n",
    "# If 0.5B not available, use 1.5B\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "fallback_model = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "server_process = None\n",
    "\n",
    "def start_vllm(model):\n",
    "    global server_process\n",
    "    server_process = subprocess.Popen([\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", model,\n",
    "        \"--host\", \"0.0.0.0\",\n",
    "        \"--port\", \"8000\",\n",
    "        \"--max-model-len\", \"2048\",\n",
    "        \"--dtype\", \"float16\",\n",
    "        \"--gpu-memory-utilization\", \"0.9\"\n",
    "    ])\n",
    "    time.sleep(30)  # Wait longer for model download\n",
    "    return server_process\n",
    "\n",
    "# Try 0.5B first\n",
    "try:\n",
    "    print(\"Starting vLLM with Qwen2.5-0.5B-Instruct...\")\n",
    "    start_vllm(model_name)\n",
    "    # Quick health check\n",
    "    import requests\n",
    "    time.sleep(5)\n",
    "    if requests.get(\"http://localhost:8000/health\").status_code == 200:\n",
    "        print(\"vLLM server started successfully with 0.5B model.\")\n",
    "    else:\n",
    "        raise Exception(\"Health check failed\")\n",
    "except Exception as e:\n",
    "    print(f\"0.5B failed: {e}. Trying 1.5B...\")\n",
    "    if server_process:\n",
    "        server_process.terminate()\n",
    "    start_vllm(fallback_model)\n",
    "    print(\"vLLM server started with 1.5B model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Monitoring with nvidia-smi (run this in a separate cell or loop)\n",
    "# For continuous monitoring, run: !watch -n 1 nvidia-smi\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous GPU monitoring in a loop (stop with interrupt)\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def monitor_gpu():\n",
    "    while True:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "        time.sleep(5)  # Update every 5 seconds\n",
    "\n",
    "monitor_thread = threading.Thread(target=monitor_gpu, daemon=True)\n",
    "monitor_thread.start()\n",
    "print(\"GPU monitoring started. Interrupt kernel to stop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the server\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/v1/chat/completions\",\n",
    "    json={\n",
    "        \"model\": \"qwen2.5\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.8\n",
    "    }\n",
    ")\n",
    "if response.status_code == 200:\n",
    "    print(\"Test successful:\")\n",
    "    print(response.json()['choices'][0]['message']['content'])\n",
    "else:\n",
    "    print(f\"Test failed: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Replace `YOUR_NGROK_AUTH_TOKEN` with your actual ngrok token.\n",
    "2. Run the cells in order. The server will download the model (may take a few minutes).\n",
    "3. Note the public URL printed (e.g., https://abc.ngrok.io). Set `export VLLM_BASE_URL=https://abc.ngrok.io/v1` in your local terminal.\n",
    "4. For GPU monitoring, run the nvidia-smi cell or the continuous monitor. (nvtop can be installed with !apt install nvtop, but nvidia-smi is more reliable in Colab.)\n",
    "5. In local code, call `configure_api_client(dry_run=False, base_url=os.environ.get('VLLM_BASE_URL'))` before running the debate tournament.\n",
    "6. Keep this Colab runtime active while running local code. Use T4 GPU runtime for best performance.\n",
    "7. To stop: Interrupt kernel and run `server_process.terminate()` if needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
