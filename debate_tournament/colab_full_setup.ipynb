{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Debate Tournament Setup on Colab with vLLM\n",
    "\n",
    "This notebook sets up the entire debate tournament system on Colab T4 GPU, including vLLM server, code download, and tournament execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install vllm pyngrok torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install tqdm nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your repo URL)\n",
    "!git clone https://github.com/yourusername/conversation_games.git\n",
    "%cd conversation_games/debate_tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ngrok (replace YOUR_NGROK_AUTH_TOKEN with your actual token)\n",
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")  # Get from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "\n",
    "# Start ngrok tunnel for port 8000\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"Public URL: {public_url}\")\n",
    "vllm_base_url = f\"{public_url}/v1\"\n",
    "print(f\"Set VLLM_BASE_URL={vllm_base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and run vLLM server with Qwen 0.5B model\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "fallback_model = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "server_process = None\n",
    "\n",
    "def start_vllm(model):\n",
    "    global server_process\n",
    "    server_process = subprocess.Popen([\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", model,\n",
    "        \"--host\", \"0.0.0.0\",\n",
    "        \"--port\", \"8000\",\n",
    "        \"--max-model-len\", \"2048\",\n",
    "        \"--dtype\", \"float16\",\n",
    "        \"--gpu-memory-utilization\", \"0.9\"\n",
    "    ])\n",
    "    time.sleep(30)  # Wait for model download\n",
    "    return server_process\n",
    "\n",
    "# Try 0.5B first\n",
    "try:\n",
    "    print(\"Starting vLLM with Qwen2.5-0.5B-Instruct...\")\n",
    "    start_vllm(model_name)\n",
    "    # Quick health check\n",
    "    import requests\n",
    "    time.sleep(5)\n",
    "    if requests.get(\"http://localhost:8000/health\").status_code == 200:\n",
    "        print(\"vLLM server started successfully with 0.5B model.\")\n",
    "    else:\n",
    "        raise Exception(\"Health check failed\")\n",
    "except Exception as e:\n",
    "    print(f\"0.5B failed: {e}. Trying 1.5B...\")\n",
    "    if server_process:\n",
    "        server_process.terminate()\n",
    "    start_vllm(fallback_model)\n",
    "    print(\"vLLM server started with 1.5B model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Monitoring with nvidia-smi\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the server\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/v1/chat/completions\",\n",
    "    json={\n",
    "        \"model\": \"qwen2.5\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.8\n",
    "    }\n",
    ")\n",
    "if response.status_code == 200:\n",
    "    print(\"Test successful:\")\n",
    "    print(response.json()['choices'][0]['message']['content'])\n",
    "else:\n",
    "    print(f\"Test failed: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the debate tournament\n",
    "import os\n",
    "os.environ['VLLM_BASE_URL'] = vllm_base_url\n",
    "\n",
    "# Import and configure\n",
    "from core.api_client import configure_api_client\n",
    "configure_api_client(dry_run=False, base_url=vllm_base_url)\n",
    "\n",
    "# Run tournament\n",
    "!python main.py --debater1-type true-mcts --debater2-type prompt-mcts --debater1-iterations 10 --debater2-iterations 5 --max-turns 2 --max-debate-depth 4 --output tournament_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "!cat tournament_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Replace `YOUR_NGROK_AUTH_TOKEN` with your actual ngrok token.\n",
    "2. Replace the git clone URL with your repository URL.\n",
    "3. Run all cells in order. The model download may take several minutes.\n",
    "4. The tournament will run automatically in the last cell.\n",
    "5. Results will be saved to `tournament_output.txt` and displayed.\n",
    "6. Keep the runtime active while running the tournament.\n",
    "7. Use T4 GPU runtime for best performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
