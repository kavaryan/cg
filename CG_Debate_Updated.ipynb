{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E61NUJdWsSmz",
        "outputId": "bf5ef538-d1be-4446-c500-0ed49a3737cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m122.9/129.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hGovernments should ban fossil-fuel car… | BASELINE vs BASELINE | 6 debates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Governments should ban fossil-fuel car… | MCTS vs BASELINE | 6 debates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Governments should ban fossil-fuel car… | BASELINE vs MCTS | 6 debates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Universal basic income should replace … | BASELINE vs BASELINE | 6 debates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Universal basic income should replace … | MCTS vs BASELINE | 6 debates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Universal basic income should replace … | BASELINE vs MCTS | 6 debates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Win-rate summary =====\n",
            "Governments should ban fossil-fuel car…  BASELINE vs BASELINE 50.0%\n",
            "Governments should ban fossil-fuel car…  MCTS vs BASELINE     100.0%\n",
            "Governments should ban fossil-fuel car…  BASELINE vs MCTS     100.0%\n",
            "Universal basic income should replace …  BASELINE vs BASELINE 33.3%\n",
            "Universal basic income should replace …  MCTS vs BASELINE     50.0%\n",
            "Universal basic income should replace …  BASELINE vs MCTS     33.3%\n",
            "\n",
            "Sample debate – first motion (MCTS vs BASELINE)\n",
            "------------------------------------------------------------\n",
            "A: To save our planet and future generations, we must decisively transition to sustainable transportation by banning fossil-fuel cars now.\n",
            "B: While ambitious, a 2035 ban ignores the technological advancements and infrastructure changes needed for a smooth transition to sustainable mobility.\n",
            "A: Prioritizing a sustainable future by banning fossil-fuel cars ensures cleaner air, combats climate change, and secure a healthier planet for generations to come.\n",
            "B: While aiming for a cleaner future is crucial, a sudden ban on fossil-fuel cars risks crippling economies and disproportionately impacting vulnerable populations.\n",
            "A: Prioritizing a future free from fossil fuel exhaust by 2035 safeguards our planet and secures a healthy future for generations to come.\n",
            "B: A sudden ban in 2035 ignores the real-world implications of economic upheaval and essential transportation access for millions.\n",
            "\n",
            "Judge: {'winner': 'B', 'score_A': 4, 'score_B': 6, 'reason': 'B addressed practical concerns and offered a more nuanced approach.'}\n"
          ]
        }
      ],
      "source": [
        "# --- Groq key -------------------------------------------------\n",
        "API_KEY = \"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\"        #  <-- put real key\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "!pip -q install groq tqdm nest_asyncio\n",
        "\n",
        "import os, re, json, random, asyncio, time, nest_asyncio\n",
        "from tqdm import tqdm\n",
        "from groq import AsyncGroq\n",
        "nest_asyncio.apply()\n",
        "\n",
        "if not API_KEY or API_KEY == \"paste_your_groq_api_key_here\":\n",
        "    raise ValueError(\"❌  Replace API_KEY with your actual Groq key.\")\n",
        "os.environ[\"GROQ_API_KEY\"] = API_KEY\n",
        "\n",
        "# ---------- parameters ----------\n",
        "MOTIONS      = [\n",
        "    \"Governments should ban fossil-fuel cars by 2035.\",\n",
        "    \"Universal basic income should replace all means-tested welfare.\",\n",
        "]\n",
        "NUM_MATCHES  = 6          # debates per pairing × motion\n",
        "SEARCH_K     = 3          # candidates per turn for MCTS\n",
        "MODEL_NAME   = \"gemma2-9b-it\"\n",
        "SLEEP_SEC    = 2          # pause between debates (≈ 15 req/min incl. judge+score)\n",
        "random.seed(42)\n",
        "\n",
        "client = AsyncGroq(api_key=API_KEY)\n",
        "\n",
        "# ---------- async helper ----------\n",
        "async def gchat(messages, temp=0.8, max_tok=256):\n",
        "    rsp = await client.chat.completions.create(\n",
        "        model       = MODEL_NAME,\n",
        "        messages    = messages,\n",
        "        temperature = temp,\n",
        "        max_tokens  = max_tok,\n",
        "    )\n",
        "    return rsp.choices[0].message.content.strip()\n",
        "\n",
        "run = lambda coro: asyncio.get_event_loop().run_until_complete(coro)\n",
        "\n",
        "# ---------- prompt builders ----------\n",
        "def debater_prompt(side, motion):\n",
        "    stance = \"FOR\" if side==\"pro\" else \"AGAINST\"\n",
        "    return [\n",
        "        {\"role\":\"system\", \"content\":\n",
        "         f\"You are a persuasive debater arguing {stance}:\\n\\\"{motion}\\\"\"},\n",
        "        {\"role\":\"user\",   \"content\":\n",
        "         \"Write ONE compelling sentence (≤25 words).\"}\n",
        "    ]\n",
        "\n",
        "def scorer_prompt(sentence, side, motion):\n",
        "    stance = \"FOR\" if side==\"pro\" else \"AGAINST\"\n",
        "    return [\n",
        "        {\"role\":\"system\", \"content\":\"Rate persuasiveness 0-10; answer with one integer.\"},\n",
        "        {\"role\":\"user\", \"content\":\n",
        "         f\"Motion: {motion}\\nSentence: {sentence}\\nRate for {stance} side.\"}\n",
        "    ]\n",
        "\n",
        "def score_sentence(sent, side, motion):\n",
        "    out = run(gchat(scorer_prompt(sent, side, motion), temp=0, max_tok=8))\n",
        "    m = re.search(r\"\\d+\", out)\n",
        "    return int(m.group()) if m else 5\n",
        "\n",
        "JUDGE_SYS = (\n",
        " \"You are the sole judge of a three-round debate.\\n\"\n",
        " 'Return JSON: {\"winner\":\"A|B|draw\",\"score_A\":0-10,\"score_B\":0-10,\"reason\":\"<20 words>\"}'\n",
        ")\n",
        "async def judge_async(text):\n",
        "    msgs=[{\"role\":\"system\",\"content\":JUDGE_SYS},\n",
        "          {\"role\":\"user\",\"content\":text}]\n",
        "    for _ in range(2):\n",
        "        raw = await gchat(msgs, temp=0, max_tok=96)\n",
        "        m = re.search(r\"\\{.*\\}\", raw, re.S)\n",
        "        if m:\n",
        "            try: return json.loads(m.group())\n",
        "            except: pass\n",
        "        msgs += [{\"role\":\"assistant\",\"content\":raw},\n",
        "                 {\"role\":\"user\",\"content\":\"Please correct to valid JSON only.\"}]\n",
        "    return {\"winner\":\"draw\",\"score_A\":5,\"score_B\":5,\"reason\":\"parse_fail\"}\n",
        "judge = lambda t: run(judge_async(t))\n",
        "\n",
        "# ---------- agents ----------\n",
        "def baseline(side, motion):\n",
        "    return lambda *_: run(gchat(debater_prompt(side, motion), temp=0.9, max_tok=40))\n",
        "\n",
        "def mcts(side, motion, k=SEARCH_K):\n",
        "    def fn(_hist,_turn):\n",
        "        best, best_s = \"\", -1\n",
        "        for _ in range(k):\n",
        "            cand = run(gchat(debater_prompt(side, motion), temp=1.2, max_tok=40))\n",
        "            sc   = score_sentence(cand, side, motion)\n",
        "            if sc > best_s: best_s, best = sc, cand\n",
        "        return best\n",
        "    return fn\n",
        "\n",
        "# ---------- debate loop ----------\n",
        "def play(motion, pro_bot, con_bot):\n",
        "    log=[]\n",
        "    for turn in range(3):\n",
        "        log.append(f\"A: {pro_bot(log,turn)}\")\n",
        "        log.append(f\"B: {con_bot(log,turn)}\")\n",
        "    verdict = judge(\"\\n\".join(log))\n",
        "    return verdict, log\n",
        "\n",
        "# ---------- tournament ----------\n",
        "rows=[]\n",
        "for motion in MOTIONS:\n",
        "    base_pro, base_con = baseline(\"pro\",motion), baseline(\"con\",motion)\n",
        "    mcts_pro, mcts_con = mcts(\"pro\",motion), mcts(\"con\",motion)\n",
        "\n",
        "    PAIRS = [(\"BASELINE vs BASELINE\", base_pro, base_con),\n",
        "             (\"MCTS vs BASELINE\",     mcts_pro, base_con),\n",
        "             (\"BASELINE vs MCTS\",     base_pro, mcts_con)]\n",
        "\n",
        "    for label,PRO,CON in PAIRS:\n",
        "        wins=0\n",
        "        print(f\"{motion[:38]}… | {label} | {NUM_MATCHES} debates\")\n",
        "        for _ in tqdm(range(NUM_MATCHES), leave=False):\n",
        "            ver,_ = play(motion, PRO, CON)\n",
        "            wins += (ver[\"winner\"]==\"A\")\n",
        "            time.sleep(SLEEP_SEC)                   # stay within free rate limit\n",
        "        rows.append((motion[:38]+\"…\"*(len(motion)>38), label, wins/NUM_MATCHES))\n",
        "\n",
        "# ---------- results ----------\n",
        "print(\"\\n===== Win-rate summary =====\")\n",
        "for m,l,w in rows:\n",
        "    print(f\"{m:<40s} {l:<20s} {w:5.1%}\")\n",
        "\n",
        "print(\"\\nSample debate – first motion (MCTS vs BASELINE)\\n\"+\"-\"*60)\n",
        "v, smp = play(MOTIONS[0], mcts(\"pro\", MOTIONS[0]), baseline(\"con\", MOTIONS[0]))\n",
        "print(\"\\n\".join(smp)); print(\"\\nJudge:\", v)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yf4Jj_h1xm4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}